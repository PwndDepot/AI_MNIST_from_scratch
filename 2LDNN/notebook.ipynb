{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "import numpy as np\n",
    "from ai_utils import ai_utils\n",
    "        \n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, m_input_size, hidden_size, n_output_size, x_train, y_train, layers = None):\n",
    "        if not layers:\n",
    "            raise NotImplementedError(\"Nueral network incomplete (missing layers)\")\n",
    "        self.m_input_size = m_input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_output_size = n_output_size\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward_propagate(self, input_data):\n",
    "        output_layer1 = self.layers[0].forward_propagate(input_data, activation_function=ai_utils.ActivationFunctions.relu)\n",
    "        output_layer2 = self.layers[1].forward_propagate(output_layer1, activation_function=ai_utils.ActivationFunctions.softmax)\n",
    "        return output_layer2\n",
    "        \n",
    "    \"\"\" def backward(self, dvalues):\n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \"\"\"\n",
    "    \n",
    "    def backward_propagate(self, y_true, y_pred, learning_rate, activation_function_derivative=ai_utils.ActivationFunctions.softmax, loss_function_derivative=ai_utils.LossFunctions.categorical_cross_entropy_derivative):\n",
    "        # gradient of categorical cross-entropy loss and softmax\n",
    "        output_error = y_pred - y_true\n",
    "        # second layer first\n",
    "        error_layer2 = self.layers[1].backward_propagate(output_error, learning_rate)\n",
    "        # first layer\n",
    "        self.layers[0].backward_propagate(error_layer2, learning_rate)\n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs = 50, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct_predictions = 0\n",
    "\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                x = x.reshape(x.shape[0], -1)\n",
    "                # forward propagation\n",
    "                y_pred = self.forward_propagate(x)\n",
    "\n",
    "                # calculate loss (MSE for example)\n",
    "                loss = ai_utils.LossFunctions.mse_loss(y, y_pred)\n",
    "                total_loss += loss\n",
    "\n",
    "                # Backward pass and update weights\n",
    "                self.backward_propagate(y, y_pred, learning_rate)\n",
    "\n",
    "                # Calculate accuracy (for classification tasks)\n",
    "                if np.argmax(output) == np.argmax(y):\n",
    "                    correct_predictions += 1\n",
    "\n",
    "            # Calculate average loss and accuracy for the epoch\n",
    "            average_loss = total_loss / len(x_train)\n",
    "            accuracy = correct_predictions / len(x_train)\n",
    "\n",
    "            # Print the metrics\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \"\"\" # TODO: change to enumerate layers\n",
    "        lowest_loss = 9999999 # some initial value\n",
    "\n",
    "        for epoch in range(10000):\n",
    "\n",
    "            layer1_output = self.layers[0].forward_propagate(self.data, activation_function=ai_utils.ActivationFunctions.relu)\n",
    "            layer2_output = self.layers[1].forward_propagate(layer1_output, activation_function=ai_utils.ActivationFunctions.softmax)\n",
    "\n",
    "            loss = ai_utils.LossFunctions.categorical_cross_entropy(layer2_output, self.labels)\n",
    "\n",
    "            predictions = np.argmax(loss, axis=0)\n",
    "            check_labels = None\n",
    "            check_labels = self.labels.copy()\n",
    "            if len(check_labels.shape) == 2:\n",
    "                check_labels = np.argmax(check_labels, axis=1)\n",
    "            accuracy = np.mean(predictions == check_labels)\n",
    "            \n",
    "            # backward propagate\n",
    "            loss_dinputs = ai_utils.LossFunctions.categorical_cross_entropy_derivative(loss, self.labels)\n",
    "            self.layers[1].backward(loss_dinputs)\n",
    "            loss_dinputs = ai_utils.ActivationFunctions.relu(self.layers[1].) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class DenseLayer(object):\n",
    "    def __init__(self, m_input_size, n_output_size):\n",
    "        self.m_input_size = m_input_size\n",
    "        self.n_output_size = n_output_size\n",
    "        self.weights = np.random.randn(m_input_size, n_output_size) * 0.01 # multiply by a small number so generated numbers are smaller, improves optimization when training\n",
    "        self.biases = np.zeros((1, n_output_size))\n",
    "        self.input = None\n",
    "        self.gradient_weights = None\n",
    "        self.gradient_biases = None\n",
    "\n",
    "    def forward_propagate(self, input_data, activation_function=ai_utils.ActivationFunctions.relu):\n",
    "        self.input = input_data\n",
    "        self.output = activation_function(np.dot(input_data, self.weights) + self.biases)\n",
    "        if activation_function:\n",
    "            return activation_function(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_propagate(self, output_error, learning_rate):\n",
    "        if self.input.ndim == 1:\n",
    "            self.input = self.input.reshape(1, -1)\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        self.gradient_weights = np.dot(self.input.T, output_error)\n",
    "        self.gradient_biases = np.sum(output_error, axis=0, keepdims=True)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * self.gradient_weights\n",
    "        self.biases -= learning_rate * self.gradient_biases\n",
    "\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4704,1) and (784,64) not aligned: 1 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\travi\\Dropbox\\Notes\\School\\Artificial Neural Networks\\HW2_python_attempt\\2LDNN\\notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m data \u001b[39m=\u001b[39m Input(X_train_flattened, y_train_encoded)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model \u001b[39m=\u001b[39m Model(input_size, hidden_size, output_size, X_train_flattened, y_train, layers)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_flattened, y_train_encoded)\n",
      "\u001b[1;32mc:\\Users\\travi\\Dropbox\\Notes\\School\\Artificial Neural Networks\\HW2_python_attempt\\2LDNN\\notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape((x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# Forward propagation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_propagate(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_propagate(x)\n",
      "\u001b[1;32mc:\\Users\\travi\\Dropbox\\Notes\\School\\Artificial Neural Networks\\HW2_python_attempt\\2LDNN\\notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_propagate\u001b[39m(\u001b[39mself\u001b[39m, input_data):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     output_layer1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mforward_propagate(input_data, activation_function\u001b[39m=\u001b[39;49mai_utils\u001b[39m.\u001b[39;49mActivationFunctions\u001b[39m.\u001b[39;49mrelu)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     output_layer2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mforward_propagate(output_layer1, activation_function\u001b[39m=\u001b[39mai_utils\u001b[39m.\u001b[39mActivationFunctions\u001b[39m.\u001b[39msoftmax)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output_layer2\n",
      "\u001b[1;32mc:\\Users\\travi\\Dropbox\\Notes\\School\\Artificial Neural Networks\\HW2_python_attempt\\2LDNN\\notebook.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_propagate\u001b[39m(\u001b[39mself\u001b[39m, input_data, activation_function\u001b[39m=\u001b[39mai_utils\u001b[39m.\u001b[39mActivationFunctions\u001b[39m.\u001b[39mrelu):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput \u001b[39m=\u001b[39m input_data\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m activation_function(np\u001b[39m.\u001b[39;49mdot(input_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m     \u001b[39mif\u001b[39;00m activation_function:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/travi/Dropbox/Notes/School/Artificial%20Neural%20Networks/HW2_python_attempt/2LDNN/notebook.ipynb#W1sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m activation_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4704,1) and (784,64) not aligned: 1 (dim 1) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "# initialize the neural network with the size of the input layer, hidden layer, and output layer.\n",
    "input_size = 28 * 28  # for MNIST, each image is 28x28 pixels\n",
    "hidden_size = 64  # arbitrary number\n",
    "output_size = 10  # MNIST has 10 classes (numbers 0-9)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# preprocess images\n",
    "# flatten\n",
    "X_train_flattened = X_train.reshape((X_test.shape[0], -1))\n",
    "#X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# normalize the data\n",
    "X_train_flattened = X_train_flattened.astype('float32') / 255\n",
    "#X_test_flattened = X_test_flattened.astype('float32') / 255.0\n",
    "\n",
    "# convert labels to one-hot encoding using the custom function\n",
    "y_train_encoded = to_categorical(y_train, num_classes=10)\n",
    "#y_test_encoded = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "layers = [DenseLayer(input_size, hidden_size), DenseLayer(hidden_size, output_size)]\n",
    "\n",
    "data = Input(X_train_flattened, y_train_encoded)\n",
    "\n",
    "model = Model(input_size, hidden_size, output_size, X_train_flattened, y_train, layers)\n",
    "\n",
    "model.fit(X_train_flattened, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02306606 0.30160894 0.9097127 ]\n",
      "[0.80744778 0.19457231 0.28212657]\n",
      "[0.25028443 0.3662832  0.27355988]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Input:\n",
    "    def __init__(self, data, batch_size = 16):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.index = len(data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index == 0:\n",
    "            return None\n",
    "        self.index -= 1\n",
    "        return self.data[self.index]\n",
    "\n",
    "\n",
    "random_array = np.random.rand(3, 3)\n",
    "\n",
    "data = Input(random_array)\n",
    "\n",
    "print(str(data.__next__()))\n",
    "print(str(data.__next__()))\n",
    "print(str(data.__next__()))\n",
    "print(str(data.__next__()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
